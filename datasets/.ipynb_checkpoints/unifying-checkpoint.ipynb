{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unify dataset format\n",
    "\n",
    "The dataset has a list of items in json, where each item contains (in order):\n",
    "```json\n",
    "{\n",
    "   \"text\": \"(optional) str\"\n",
    "   \"category\": \"str:(domain,intent,etc.)\"\n",
    "   \"tokens\":\"list[str]\",\n",
    "   \"token_spans\": \"(optional) list[tuple(int,int)]\",\n",
    "   \"slot_tags\": \"list[str]:BIO or BIEO or BIEOS\"\n",
    "}\n",
    "```\n",
    "\n",
    "we adopt bert vocab file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slot-Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snip\n",
    "\n",
    "Download: https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './original/SNIP/2017-06-custom-intent-engines/'\n",
    "train_paths = []\n",
    "valid_paths = []\n",
    "for path in os.listdir(ROOT):\n",
    "    path = os.path.join(ROOT, path)\n",
    "    if not os.path.isdir(path):\n",
    "        continue\n",
    "    for sub_path in os.listdir(path):\n",
    "        if re.match('^train.*full.json$', sub_path):\n",
    "            sub_path = os.path.join(path, sub_path)\n",
    "            train_paths.append(sub_path)\n",
    "        elif re.match('^valid.*json$', sub_path):\n",
    "            sub_path = os.path.join(path, sub_path)\n",
    "            valid_paths.append(sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(paths):\n",
    "    dataset = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf-8', errors=\"ignore\") as f:\n",
    "            json_list = list(json.load(f).values())[0]\n",
    "        for item in json_list:\n",
    "                \n",
    "            text = ''\n",
    "            slots = []\n",
    "            for sub_item in item['data']:\n",
    "                _text = strip_accents(sub_item['text']).lower()\n",
    "\n",
    "                if 'entity' in sub_item:\n",
    "                    slots.append((sub_item['entity'], (len(text), len(text)+len(_text))))\n",
    "                    text += _text\n",
    "                else:\n",
    "                    text += _text\n",
    "            tokens, token_spans = tokenize_with_span(text)\n",
    "            tags = ['O'] * len(tokens)\n",
    "            \n",
    "            if_add = True\n",
    "            for slot_name, slot_span in slots:\n",
    "                        \n",
    "                i_slot_token = -1\n",
    "                j_slot_token = -1\n",
    "                for i, token_span in enumerate(token_spans):\n",
    "                    if token_span[0] == slot_span[0]:\n",
    "                        i_slot_token = i\n",
    "                    if token_span[1] == slot_span[1] or token_span[1]+1 == slot_span[1]:\n",
    "                        j_slot_token = i + 1\n",
    "                if i_slot_token < 0 or j_slot_token < 0:\n",
    "                    if_add = False\n",
    "                    print('warning: not found.')\n",
    "                    print(slot_name, slot_span)\n",
    "                    print(text)\n",
    "                    print([(token, token_span) for token, token_span in zip(tokens, token_spans)])\n",
    "                else:\n",
    "                    add_tags(tags, attr=slot_name, span=[i_slot_token, j_slot_token])\n",
    "\n",
    "            if if_add:\n",
    "                dataset.append({\n",
    "                   \"text\": text,\n",
    "                   \"category\": 'SNIPS' + '__' + split_path(path)[-2],\n",
    "                   \"tokens\": [re.sub('\\d', '0', t) for t in tokens],\n",
    "                   \"token_spans\": token_spans,\n",
    "                   \"slot_tags\": tags,\n",
    "                })\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: not found.\n",
      "artist (11, 23)\n",
      "live in l.ajoseph meyer please\n",
      "[('live', (0, 4)), ('in', (5, 7)), ('l', (8, 9)), ('.', (9, 10)), ('ajoseph', (10, 17)), ('meyer', (18, 23)), ('please', (24, 30))]\n",
      "warning: not found.\n",
      "timeRange (40, 46)\n",
      "what kind of weather is forecast around one pmnear vatican?\n",
      "[('what', (0, 4)), ('kind', (5, 9)), ('of', (10, 12)), ('weather', (13, 20)), ('is', (21, 23)), ('forecast', (24, 32)), ('around', (33, 39)), ('one', (40, 43)), ('pmnear', (44, 50)), ('vatican', (51, 58)), ('?', (58, 59))]\n",
      "warning: not found.\n",
      "spatial_relation (46, 50)\n",
      "what kind of weather is forecast around one pmnear vatican?\n",
      "[('what', (0, 4)), ('kind', (5, 9)), ('of', (10, 12)), ('weather', (13, 20)), ('is', (21, 23)), ('forecast', (24, 32)), ('around', (33, 39)), ('one', (40, 43)), ('pmnear', (44, 50)), ('vatican', (51, 58)), ('?', (58, 59))]\n",
      "warning: not found.\n",
      "movie_name (9, 21)\n",
      "show the sexy dance 2times at the  closest movie house\n",
      "[('show', (0, 4)), ('the', (5, 8)), ('sexy', (9, 13)), ('dance', (14, 19)), ('2times', (20, 26)), ('at', (27, 29)), ('the', (30, 33)), ('closest', (35, 42)), ('movie', (43, 48)), ('house', (49, 54))]\n",
      "warning: not found.\n",
      "object_type (21, 26)\n",
      "show the sexy dance 2times at the  closest movie house\n",
      "[('show', (0, 4)), ('the', (5, 8)), ('sexy', (9, 13)), ('dance', (14, 19)), ('2times', (20, 26)), ('at', (27, 29)), ('the', (30, 33)), ('closest', (35, 42)), ('movie', (43, 48)), ('house', (49, 54))]\n"
     ]
    }
   ],
   "source": [
    "trainset = build_dataset(train_paths)\n",
    "validset = build_dataset(valid_paths)\n",
    "\n",
    "random.shuffle(trainset)\n",
    "ratio = len(validset) / len(trainset)\n",
    "split_n = int(len(trainset)*ratio)\n",
    "trainset, testset = trainset[split_n:], trainset[:split_n]\n",
    "\n",
    "with open('./unified/train.snip.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open('./unified/valid.snip.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open('./unified/test.snip.json', 'w', encoding='utf-8') as f: # \n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude_domain: SNIPS__SearchScreeningEvent\n",
      "35\n",
      "exclude_domain: SNIPS__AddToPlaylist\n",
      "37\n",
      "exclude_domain: SNIPS__GetWeather\n",
      "35\n",
      "exclude_domain: SNIPS__BookRestaurant\n",
      "31\n",
      "exclude_domain: SNIPS__PlayMusic\n",
      "34\n",
      "exclude_domain: SNIPS__SearchCreativeWork\n",
      "39\n",
      "exclude_domain: SNIPS__RateBook\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "domains = set()\n",
    "for item in trainset:\n",
    "    domains.add(item['category'])\n",
    "\n",
    "for exclude_domain in domains:\n",
    "    _trainset = []\n",
    "    _validset = []\n",
    "    _testset = []\n",
    "    for item in trainset:\n",
    "        if item['category'] != exclude_domain:\n",
    "            _trainset.append(item)\n",
    "    for item in validset:\n",
    "        if item['category'] != exclude_domain:\n",
    "            _validset.append(item)\n",
    "    for item in testset:\n",
    "        if item['category'] != exclude_domain:\n",
    "            _testset.append(item)\n",
    "    with open(f'./unified/train.snip.no_{exclude_domain}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(_trainset, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    with open(f'./unified/valid.snip.no_{exclude_domain}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(_validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "    with open(f'./unified/test.snip.no_{exclude_domain}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(_testset, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    ###\n",
    "    count_dict = defaultdict(int)\n",
    "    for item in _trainset:\n",
    "        for tag in item['slot_tags']:\n",
    "            if tag[0] == 'B':\n",
    "                count_dict[tag[2:]] += 1\n",
    "    print(f\"exclude_domain: {exclude_domain}\")\n",
    "    print(len(count_dict))\n",
    "#     pprint(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__AddToPlaylist')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__GetWeather')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__BookRestaurant')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__PlayMusic')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__SearchCreativeWork')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchScreeningEvent', 'SNIPS__RateBook')\n",
      "39\n",
      "exclude_domain: ('SNIPS__AddToPlaylist', 'SNIPS__GetWeather')\n",
      "39\n",
      "exclude_domain: ('SNIPS__AddToPlaylist', 'SNIPS__BookRestaurant')\n",
      "39\n",
      "exclude_domain: ('SNIPS__AddToPlaylist', 'SNIPS__PlayMusic')\n",
      "39\n",
      "exclude_domain: ('SNIPS__AddToPlaylist', 'SNIPS__SearchCreativeWork')\n",
      "39\n",
      "exclude_domain: ('SNIPS__AddToPlaylist', 'SNIPS__RateBook')\n",
      "39\n",
      "exclude_domain: ('SNIPS__GetWeather', 'SNIPS__BookRestaurant')\n",
      "39\n",
      "exclude_domain: ('SNIPS__GetWeather', 'SNIPS__PlayMusic')\n",
      "39\n",
      "exclude_domain: ('SNIPS__GetWeather', 'SNIPS__SearchCreativeWork')\n",
      "39\n",
      "exclude_domain: ('SNIPS__GetWeather', 'SNIPS__RateBook')\n",
      "39\n",
      "exclude_domain: ('SNIPS__BookRestaurant', 'SNIPS__PlayMusic')\n",
      "39\n",
      "exclude_domain: ('SNIPS__BookRestaurant', 'SNIPS__SearchCreativeWork')\n",
      "39\n",
      "exclude_domain: ('SNIPS__BookRestaurant', 'SNIPS__RateBook')\n",
      "39\n",
      "exclude_domain: ('SNIPS__PlayMusic', 'SNIPS__SearchCreativeWork')\n",
      "39\n",
      "exclude_domain: ('SNIPS__PlayMusic', 'SNIPS__RateBook')\n",
      "39\n",
      "exclude_domain: ('SNIPS__SearchCreativeWork', 'SNIPS__RateBook')\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations, permutations\n",
    "\n",
    "domains = set()\n",
    "for item in trainset:\n",
    "    domains.add(item['category'])\n",
    "\n",
    "for exclude_domains in combinations(domains, 2):\n",
    "    _trainset = []\n",
    "    _validset = []\n",
    "    _testset = []\n",
    "    for item in trainset:\n",
    "        if item['category'] not in exclude_domains:\n",
    "            _trainset.append(item)\n",
    "    for item in validset:\n",
    "        if item['category'] not in exclude_domains:\n",
    "            _validset.append(item)\n",
    "    for item in testset:\n",
    "        if item['category'] not in exclude_domains:\n",
    "            _testset.append(item)\n",
    "    with open(f\"./unified/train.snip.no_{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_trainset, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    with open(f\"./unified/valid.snip.no_{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "    with open(f\"./unified/test.snip.no_{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_testset, f, sort_keys=True, indent=4)\n",
    "              \n",
    "              \n",
    "    for item in trainset:\n",
    "        if item['category'] in exclude_domains:\n",
    "            _trainset.append(item)\n",
    "    for item in validset:\n",
    "        if item['category'] in exclude_domains:\n",
    "            _validset.append(item)\n",
    "    for item in testset:\n",
    "        if item['category'] in exclude_domains:\n",
    "            _testset.append(item)\n",
    "    with open(f\"./unified/train.snip.{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_trainset, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    with open(f\"./unified/valid.snip.{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "    with open(f\"./unified/test.snip.{'_'.join(exclude_domains)}.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(_testset, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    ###\n",
    "    count_dict = defaultdict(int)\n",
    "    for item in _trainset:\n",
    "        for tag in item['slot_tags']:\n",
    "            if tag[0] == 'B':\n",
    "                count_dict[tag[2:]] += 1\n",
    "    print(f\"exclude_domain: {exclude_domains}\")\n",
    "    print(len(count_dict))\n",
    "#     pprint(count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT REST\n",
    "\n",
    "Download: https://groups.csail.mit.edu/sls/downloads/restaurant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens_tags(tokens, tags, use_tokenizer=True, digit2zero=True):\n",
    "    '''\n",
    "    tags in BIO format\n",
    "    '''\n",
    "    \n",
    "    if use_tokenizer:\n",
    "        _tokenize = tokenize\n",
    "    else:\n",
    "        _tokenize = lambda x: [x]\n",
    "        \n",
    "    if digit2zero:\n",
    "        tokens = [re.sub('\\d', '0', t) for t in tokens]\n",
    "    \n",
    "    target_tokens = []\n",
    "    target_tags = []\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag[0:2] not in ['O', 'B-', 'I-']:\n",
    "            tag = 'B-' + tag\n",
    "        for sub_token in _tokenize(token):\n",
    "            target_tokens.append(sub_token)\n",
    "            target_tags.append(tag)\n",
    "            if tag[0] == 'B':\n",
    "                tag = 'I' + tag[1:]\n",
    "    return target_tokens, target_tags\n",
    "\n",
    "def read_dataset(path, category=None, splitter='\\t', col_tag=0, col_token=1, use_tokenizer=False):\n",
    "    dataset = []\n",
    "    last_is_O = True\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                tokens, tags = convert_tokens_tags(tokens, tags, use_tokenizer)\n",
    "                item = {\n",
    "                    \"text\": None,\n",
    "                    \"category\": category,\n",
    "                    \"tokens\": tokens,\n",
    "                    \"slot_tags\": tags,\n",
    "                }\n",
    "                dataset.append(item)\n",
    "                tokens, tags = [], []\n",
    "                continue\n",
    "            tmp = line.split(splitter)\n",
    "            tag, token = tmp[col_tag], tmp[col_token]\n",
    "            if last_is_O and (tag != 'O'):\n",
    "                tag = 'B' + tag[1:]\n",
    "            last_is_O = (tag == 'O')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = read_dataset('./original/MIT-Restaurant/train.txt', category=\"MIT-Restaurant\")\n",
    "testset = read_dataset('./original/MIT-Restaurant/test.txt', category=\"MIT-Restaurant\")\n",
    "\n",
    "random.shuffle(trainset)\n",
    "ratio = len(testset) / len(trainset)\n",
    "split_n = int(len(trainset)*ratio)\n",
    "trainset, validset = trainset[split_n:], trainset[:split_n]\n",
    "\n",
    "with open('./unified/train.rest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open('./unified/valid.rest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open('./unified/test.rest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT Movie\n",
    "\n",
    "Download: https://groups.csail.mit.edu/sls/downloads/movie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'movie_eng'\n",
    "\n",
    "trainset = read_dataset('./original/MIT-Movie/engtrain.txt', category=\"MIT-Movie-Eng\")\n",
    "testset = read_dataset('./original/MIT-Movie/engtest.txt', category=\"MIT-Movie-Eng\")\n",
    "\n",
    "random.shuffle(trainset)\n",
    "ratio = len(testset) / len(trainset)\n",
    "split_n = int(len(trainset)*ratio)\n",
    "trainset, validset = trainset[split_n:], trainset[:split_n]\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'movie_trivia10k13'\n",
    "\n",
    "trainset = read_dataset('./original/MIT-Movie/trivia10k13train.txt', category=\"MIT-Movie-Triv\")\n",
    "testset = read_dataset('./original/MIT-Movie/trivia10k13test.txt', category=\"MIT-Movie-Triv\")\n",
    "\n",
    "random.shuffle(trainset)\n",
    "ratio = len(testset) / len(trainset)\n",
    "split_n = int(len(trainset)*ratio)\n",
    "trainset, validset = trainset[split_n:], trainset[:split_n]\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnEM\n",
    "\n",
    "https://github.com/juand-r/entity-recognition-datasets/tree/master/data/AnEM/CONLL-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'AnEM'\n",
    "\n",
    "trainset = read_dataset('./original/ER-AnEM/AnEM.train',category='AnEM' , splitter='\\t', col_tag=-1, col_token=0)\n",
    "testset = read_dataset('./original/ER-AnEM/AnEM.test',category='AnEM' , splitter='\\t', col_tag=-1, col_token=0)\n",
    "validset = read_dataset('./original/ER-AnEM/AnEM.test',category='AnEM' , splitter='\\t', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'BTC'\n",
    "\n",
    "trainset = read_dataset('./original/ER-BTC/all.txt', category='BTC' , splitter='\\t', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'FIN5'\n",
    "\n",
    "trainset = read_dataset('./original/ER-FIN5/train.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "testset = read_dataset('./original/ER-FIN5/test.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "validset = read_dataset('./original/ER-FIN5/test.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'GUM'\n",
    "\n",
    "trainset = read_dataset('./original/ER-GUM/gum-train.conll',category=flag , splitter='\\t', col_tag=-1, col_token=0)\n",
    "testset = read_dataset('./original/ER-GUM/gum-test.conll',category=flag , splitter='\\t', col_tag=-1, col_token=0)\n",
    "validset = read_dataset('./original/ER-GUM/gum-test.conll',category=flag , splitter='\\t', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'Ritter'\n",
    "\n",
    "trainset = read_dataset('./original/ER-Ritter/train.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "testset = read_dataset('./original/ER-Ritter/test.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "validset = read_dataset('./original/ER-Ritter/dev.txt',category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiGold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'WikiGold'\n",
    "\n",
    "trainset = read_dataset('./original/ER-WikiGold/train.txt', category=flag , splitter=' ', col_tag=-1, col_token=0)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONLL2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'CONLL2003-NER'\n",
    "use_tokenizer = False\n",
    "\n",
    "trainset = read_dataset('./original/CONLL2003/train.txt',category=flag , splitter=' ', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "testset = read_dataset('./original/CONLL2003/test.txt',category=flag , splitter=' ', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "validset = read_dataset('./original/CONLL2003/valid.txt',category=flag , splitter=' ', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)\n",
    "\n",
    "    \n",
    "flag = 'CONLL2003-CHUNK'\n",
    "\n",
    "trainset = read_dataset('./original/CONLL2003/train.txt',category=flag , splitter=' ', col_tag=-2, col_token=0, use_tokenizer=use_tokenizer)\n",
    "testset = read_dataset('./original/CONLL2003/test.txt',category=flag , splitter=' ', col_tag=-2, col_token=0, use_tokenizer=use_tokenizer)\n",
    "validset = read_dataset('./original/CONLL2003/valid.txt',category=flag , splitter=' ', col_tag=-2, col_token=0, use_tokenizer=use_tokenizer)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)\n",
    "\n",
    "    \n",
    "flag = 'CONLL2003-POS'\n",
    "\n",
    "trainset = read_dataset('./original/CONLL2003/train.txt',category=flag , splitter=' ', col_tag=-3, col_token=0, use_tokenizer=use_tokenizer)\n",
    "testset = read_dataset('./original/CONLL2003/test.txt',category=flag , splitter=' ', col_tag=-3, col_token=0, use_tokenizer=use_tokenizer)\n",
    "validset = read_dataset('./original/CONLL2003/valid.txt',category=flag , splitter=' ', col_tag=-3, col_token=0, use_tokenizer=use_tokenizer)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 'ONTO'\n",
    "use_tokenizer = False\n",
    "\n",
    "trainset = read_dataset('./original/ontonotes/onto.train.ner',category=flag , splitter='\\t', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "testset = read_dataset('./original/ontonotes/onto.test.ner',category=flag , splitter='\\t', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "validset = read_dataset('./original/ontonotes/onto.development.ner',category=flag , splitter='\\t', col_tag=-1, col_token=0, use_tokenizer=use_tokenizer)\n",
    "\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(trainset, f, sort_keys=True, indent=4)\n",
    "\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(validset, f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(testset, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149153\n",
      "20032\n",
      "16069\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))\n",
    "print(len(validset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACE05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_ace_data(path):\n",
    "    \n",
    "#     valid_ett = {\n",
    "#         'FAC', 'GPE', 'LOC', 'ORG', 'LOC', 'PER', 'VEH', 'WEA'\n",
    "#     }\n",
    "    \n",
    "#     with open(path) as f:\n",
    "#         dataset = json.load(f)\n",
    "\n",
    "#     dataset = [\n",
    "#         {\n",
    "#             'text': item['sentence'],\n",
    "#             'tokens': item['words'],\n",
    "#             'entities': [\n",
    "#                 {\n",
    "#                     'entity_type': entity_item['entity-type'].split(':')[0],\n",
    "#                     'span': (entity_item['start'], entity_item['end'],),\n",
    "#                 } for entity_item in item['golden-entity-mentions'] if entity_item['entity-type'].split(':')[0] in valid_ett\n",
    "#             ]\n",
    "#         } for item in dataset\n",
    "#     ]\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = read_ace_data('./original/ace_2005_td_v7/output/test.json')\n",
    "# valid = read_ace_data('./original/ace_2005_td_v7/output/dev.json')\n",
    "# train = read_ace_data('./original/ace_2005_td_v7/output/train.json')\n",
    "\n",
    "# flag = 'ACE05_OLD'\n",
    "# with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(test, f, sort_keys=True, indent=4)\n",
    "# with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(valid, f, sort_keys=True, indent=4)\n",
    "# with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(train, f, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nest_data(path, n_line_one_item=3, \n",
    "                   tokens_line=0, etts_line=1, pos_line=None,\n",
    "                   end_plus=0):\n",
    "    items = []\n",
    "    with open(path) as f:\n",
    "        last = {}\n",
    "        for i, line in enumerate(f):\n",
    "            if i % n_line_one_item == tokens_line:\n",
    "                last['tokens'] = line.strip().split(' ')\n",
    "#                 if len(line.strip().split(' ')) != len(line.strip().split()):\n",
    "#                     print(line.strip().split(' '))\n",
    "#                     print(line.strip().split())\n",
    "            elif pos_line is not None and i % n_line_one_item == pos_line:\n",
    "                last['pos'] = line.strip().split(' ')\n",
    "                assert len(last['pos']) == len(last['tokens'])\n",
    "            elif i % n_line_one_item == etts_line:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    entities = {\n",
    "                        (\n",
    "                            txt.split()[1], tuple(int(p) for p in txt.split()[0].split(','))\n",
    "                        ) for txt in line.split('|')\n",
    "                    }\n",
    "                    \n",
    "                    last['entities'] = [\n",
    "                        {\n",
    "                            'entity_type': entity[0], \n",
    "                            'span': (entity[1][0], entity[1][1] + int(end_plus)),\n",
    "                        } for entity in entities\n",
    "                    ]\n",
    "                    \n",
    "                else:\n",
    "                    last['entities'] = []\n",
    "            elif i % n_line_one_item == n_line_one_item - 1:\n",
    "                items.append(last)\n",
    "                last = {}\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_of_ett_by_len(dataset):\n",
    "    count = defaultdict(int)\n",
    "    for item in dataset:\n",
    "        entities = item['entities']\n",
    "        for i, entity in enumerate(entities):\n",
    "            start, end = entity['span']\n",
    "            count[end-start] += 1\n",
    "    n_total = sum(count.values())\n",
    "    for k in sorted(count.keys()):\n",
    "        print(f\"{k}: {count[k]/n_total*100:.2f}%, {count[k]}\")\n",
    "        \n",
    "def print_num_of_ett_by_type(dataset):\n",
    "    count = defaultdict(int)\n",
    "    for item in dataset:\n",
    "        entities = item['entities']\n",
    "        entity_by_span = defaultdict(set)\n",
    "        for entity in entities:\n",
    "            start, end, ett_type = *entity['span'], entity['entity_type']\n",
    "            entity_by_span[(start, end)].add(ett_type)\n",
    "        for (start, end), ett_types in entity_by_span.items():\n",
    "            ett_type = '|'.join(sorted(list(ett_types)))\n",
    "            count[ett_type] += 1\n",
    "            \n",
    "    print(len(count))\n",
    "    n_total = sum(count.values())\n",
    "    for k, v in sorted(count.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{k}: {count[k]/n_total*100:.2f}%, {count[k]}\")\n",
    "    \n",
    "    return [count[k]/n_total*100 for k,v in sorted(count.items(), key=lambda x: -x[1])]\n",
    "        \n",
    "        \n",
    "def print_overlapping_num_of_ett_by_len(dataset):\n",
    "    \n",
    "    count = defaultdict(int)\n",
    "    n_total = 0\n",
    "    for item in dataset:\n",
    "        entities = item['entities']\n",
    "        for i, entity in enumerate(entities):\n",
    "            n_total += 1\n",
    "            start, end = entity['span']\n",
    "            if is_overlapping_list(entity['span'], [\n",
    "                entities[j]['span'] for j in range(len(entities)) \\\n",
    "                    if i!=j #and entities[j]['span'][1] - entities[j]['span'][0] >= end - start\n",
    "            ]):\n",
    "                count[end-start] += 1\n",
    "    #n_total = sum(count.values())\n",
    "    for k in sorted(count.keys()):\n",
    "        print(f\"{k}: {count[k]/n_total*100:.2f}%, {count[k]}\")\n",
    "        \n",
    "    return [count[k]/n_total*100 for k,v in sorted(count.items(), key=lambda x: -x[1])]\n",
    "        \n",
    "def is_overlapping(span_a, span_b):\n",
    "    if span_a[0] <= span_b[0] < span_a[1] or span_a[0] < span_b[1] <= span_a[1]:\n",
    "        return True\n",
    "    span_a, span_b = span_b, span_a\n",
    "    if span_a[0] <= span_b[0] < span_a[1] or span_a[0] < span_b[1] <= span_a[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_nesting(span_a, span_b):\n",
    "    if span_a[0] <= span_b[0] < span_a[1] and span_a[0] < span_b[1] <= span_a[1]:\n",
    "        return True\n",
    "    span_a, span_b = span_b, span_a\n",
    "    if span_a[0] <= span_b[0] < span_a[1] and span_a[0] < span_b[1] <= span_a[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_overlapping_list(span_a, span_list):\n",
    "    for span_b in span_list:\n",
    "        if is_overlapping(span_a, span_b):\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def has_overlapping_but_not_nested(l):\n",
    "    for last, curr in combinations(l, 2):\n",
    "        if (curr[2][0] < last[2][0] < curr[2][1] and last[2][1] > curr[2][1]) or \\\n",
    "            (last[2][0] < curr[2][0] < last[2][1] and curr[2][1] > last[2][1]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_overlapping(l):\n",
    "    for last, curr in combinations(l, 2):\n",
    "        if is_overlapping(last[2], curr[2]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 6198\n",
      "# valid: 742\n",
      "# test: 809\n"
     ]
    }
   ],
   "source": [
    "test = read_nest_data('./original/NEST_ACE2004/ace2004.test')\n",
    "valid = read_nest_data('./original/NEST_ACE2004/ace2004.dev')\n",
    "train = read_nest_data('./original/NEST_ACE2004/ace2004.train')\n",
    "\n",
    "flag = 'ACE04'\n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(valid, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, sort_keys=True, indent=4)\n",
    "    \n",
    "print(f\"# train: {len(train)}\")\n",
    "print(f\"# valid: {len(valid)}\")\n",
    "print(f\"# test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 7285\n",
      "# valid: 968\n",
      "# test: 1058\n"
     ]
    }
   ],
   "source": [
    "test = read_nest_data('./original/NEST_ACE2005/ace2005.test')\n",
    "valid = read_nest_data('./original/NEST_ACE2005/ace2005.dev')\n",
    "train = read_nest_data('./original/NEST_ACE2005/ace2005.train')\n",
    "\n",
    "flag = 'ACE05'\n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(valid, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, sort_keys=True, indent=4)\n",
    "    \n",
    "print(f\"# train: {len(train)}\")\n",
    "print(f\"# valid: {len(valid)}\")\n",
    "print(f\"# test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "31\n",
      "27\n",
      "===\n",
      "2797 7285 0.383939601921757\n",
      "352 968 0.36363636363636365\n",
      "339 1058 0.32041587901701324\n",
      "===\n",
      "9946 24700 0.4026720647773279\n",
      "1191 3218 0.37010565568676196\n",
      "1179 3029 0.3892373720699901\n"
     ]
    }
   ],
   "source": [
    "# # 最长的实体\n",
    "# for dataset in [ train, valid, test]:\n",
    "#     mlen = max([max([0, *[e['span'][1]-e['span'][0] for e in item['entities']]]) for item in dataset])\n",
    "#     print(mlen)\n",
    "    \n",
    "# print('===')\n",
    "# # 数据集统计\n",
    "# for dataset in [train, valid, test]:\n",
    "#     n = n_total = 0\n",
    "#     for item in dataset:\n",
    "#         n_total += 1\n",
    "#         if has_overlapping([(e['entity_type'], e['entity_type'], e['span']) for e in item['entities']]):\n",
    "#             n += 1\n",
    "#     print(n, n_total, n / n_total)\n",
    "# print('===')\n",
    "# for dataset in [train, valid, test]:\n",
    "#     n = n_total = 0\n",
    "#     for item in dataset:\n",
    "#         entities = item['entities']\n",
    "#         n_total += len(entities)\n",
    "#         for i, entity in enumerate(entities):\n",
    "#             if is_overlapping_list(entity['span'], [\n",
    "#                 entities[j]['span'] for j in range(len(entities)) \\\n",
    "#                     if i!=j #and entities[j]['span'][1] - entities[j]['span'][0] >= end - start\n",
    "#             ]):\n",
    "#                 n += 1\n",
    "#     print(n, n_total, n / n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 15022\n",
      "# valid: 1669\n",
      "# test: 1855\n"
     ]
    }
   ],
   "source": [
    "test = read_nest_data('./original/NEST_GENIA/genia.test')\n",
    "valid = read_nest_data('./original/NEST_GENIA/genia.dev')\n",
    "train = read_nest_data('./original/NEST_GENIA/genia.train')\n",
    "\n",
    "flag = 'GENIA'\n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(valid, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, sort_keys=True, indent=4)\n",
    "    \n",
    "print(f\"# train: {len(train)}\")\n",
    "print(f\"# valid: {len(valid)}\")\n",
    "print(f\"# test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 43457\n",
      "# valid: 1989\n",
      "# test: 3762\n"
     ]
    }
   ],
   "source": [
    "test = read_nest_data('./original/NNE/test.txt', n_line_one_item=4, tokens_line=0, etts_line=2, end_plus=1)\n",
    "valid = read_nest_data('./original/NNE/dev.txt', n_line_one_item=4, tokens_line=0, etts_line=2, end_plus=1)\n",
    "train = read_nest_data('./original/NNE/train.txt', n_line_one_item=4, tokens_line=0, etts_line=2, end_plus=1)\n",
    "\n",
    "flag = 'NNE'\n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(valid, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, sort_keys=True, indent=4)\n",
    "    \n",
    "print(f\"# train: {len(train)}\")\n",
    "print(f\"# valid: {len(valid)}\")\n",
    "print(f\"# test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\n",
    "    \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\",\n",
    "    cuda_device=0,\n",
    ")\n",
    "\n",
    "predictor._tokenizer = SpacyWordSplitter(split_on_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlapping(sentences):\n",
    "    json_list = [{'sentence':s} for s in sentences]\n",
    "    objs = predictor.predict_batch_json(json_list)\n",
    "    rets = []\n",
    "    for obj in objs:\n",
    "        l = [obj['hierplane_tree']['root']]\n",
    "        ret = []\n",
    "        while l:\n",
    "            _tmp = l.pop()\n",
    "            if _tmp['nodeType'] == 'NP' and any(c['nodeType']=='SBAR' for c in _tmp['children']):\n",
    "                ret.append(_tmp)\n",
    "            if 'children' in _tmp:\n",
    "                l += _tmp['children']\n",
    "        rets.append(ret)\n",
    "    return rets\n",
    "\n",
    "def find_sub_list(main_list, sub_list):\n",
    "    for i in range(len(main_list) - len(sub_list)+1):\n",
    "        if main_list[i] != sub_list[0]:\n",
    "            continue\n",
    "        if main_list[i:i+len(sub_list)] == sub_list:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "dataset = [item for item in train+valid+test if \\\n",
    "           find_sub_list(item['tokens'], [',', 'which']) is not None or \\\n",
    "           find_sub_list(item['tokens'], [',', 'who']) is not None]\n",
    "\n",
    "rets = []\n",
    "bs = 32\n",
    "for i in tqdm(range(0, len(dataset), bs)):\n",
    "    items = dataset[i:i+bs]\n",
    "    sentences = [' '.join(item['tokens']) for item in items]\n",
    "    batch_rets = get_overlapping(sentences)\n",
    "    for item, _ret in zip(items, batch_rets):\n",
    "        if len(_ret):\n",
    "            rets.append((item, _ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2599"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_dataset = []\n",
    "for item, tree_list in rets:\n",
    "    item = deepcopy(item)\n",
    "    tokens = item['tokens']\n",
    "    text = ' '.join(tokens)\n",
    "    for tree in tree_list:\n",
    "        root_tokens = tree['word'].split()\n",
    "        if ',' in root_tokens:\n",
    "            comma = root_tokens.index(',')\n",
    "        else:\n",
    "            continue\n",
    "        NN_tokens = root_tokens[:comma]\n",
    "        for _splitter in ['\\'s', 'of']:\n",
    "            if _splitter in NN_tokens:\n",
    "                clause_tokens = root_tokens[NN_tokens.index(_splitter)+1:]\n",
    "#                 print(' '.join(NN_tokens))\n",
    "#                 print(' '.join(clause_tokens))\n",
    "#                 print('=-=')\n",
    "                break\n",
    "        else:\n",
    "            clause_tokens = root_tokens\n",
    "#             print(text)\n",
    "#             print('=-=')\n",
    "            \n",
    "        if clause_tokens[-1] in [',', '.']:\n",
    "            clause_tokens = clause_tokens[:-1]\n",
    "        \n",
    "        start = find_sub_list(tokens, NN_tokens)\n",
    "        end = start + len(NN_tokens)\n",
    "        item['entities'].append({\n",
    "            'entity_type': 'NN',\n",
    "            'span': (start, end),\n",
    "        })\n",
    "        \n",
    "        start = find_sub_list(tokens, clause_tokens)\n",
    "        end = start + len(clause_tokens)\n",
    "        item['entities'].append({\n",
    "            'entity_type': 'CLA',\n",
    "            'span': (start, end),\n",
    "        })\n",
    "            \n",
    "    overlapping_dataset.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train: 1599\n",
      "# valid: 400\n",
      "# test: 600\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(overlapping_dataset)\n",
    "valid, test, train = overlapping_dataset[:400], overlapping_dataset[400:1000], overlapping_dataset[1000:]\n",
    "\n",
    "flag = 'NNE_OL'\n",
    "with open(f'./unified/test.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/valid.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(valid, f, sort_keys=True, indent=4)\n",
    "with open(f'./unified/train.{flag}.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, sort_keys=True, indent=4)\n",
    "    \n",
    "print(f\"# train: {len(train)}\")\n",
    "print(f\"# valid: {len(valid)}\")\n",
    "print(f\"# test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
